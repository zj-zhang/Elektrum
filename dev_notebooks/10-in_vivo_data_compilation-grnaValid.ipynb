{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3149ad80",
   "metadata": {},
   "source": [
    "# Compile in vivo off-target data for transfer learning\n",
    "\n",
    "We will use the compiled data from CRISPR-Net: A Recurrent Convolutional Network Quantifies CRISPR Off-Target Activities with Mismatches and Indels, by Lin et al., *Advanced Science*, 2020.\n",
    "\n",
    "The data and code files can be downloaded from the CodeOccean pod here: https://codeocean.com/capsule/9553651/tree/v1 . Great job on making the work reproducible!\n",
    "\n",
    "Please note that `pd.read_pickle` from these pre-compiled .pkl files requires `pandas.__version__~='1.0.3'`\n",
    "\n",
    "Below is the README from `CRISPR_Net/data/`\n",
    "\n",
    "| Name | Location in data/ | Technique |with Indel| Lierature\n",
    "| ----:| :---- |----: |----: |----: |\n",
    "| Dataset I-1| Dataset I (indel&mismatch) |CIRCLE-Seq|Yes| Tsai et al., Nat Method, 2017|\n",
    "| Dataset I-2| Dataset I (indel&mismatch) |GUIDE-Seq|Yes| Listgarten et al., Nat BME, 2018 |\n",
    "| Dataset II-1| Dataset II (mismatch-only) |protein knockout detection|No| Doench et al., Nat biotech, 2016 |\n",
    "| Dataset II-2| Dataset II (mismatch-only) |PCR, Diggenome-Seq, etc|No| Haeussler et al., Genome bio, 2016|\n",
    "| Dataset II-3| Dataset II (mismatch-only) |SITE-Seq|No|Cameron et al., Nature Methods, 2017 |\n",
    "| Dataset II-4| Dataset II (mismatch-only) |GUIDE-Seq|No| Tsai et al., Nat biotech, 2015|\n",
    "| Dataset II-5| Dataset II (mismatch-only) |GUIDE-Seq|No| Kleinstiver et al., Nature, 2015|\n",
    "| Dataset II-6| Dataset II (mismatch-only) |GUIDE-Seq|No| Listgarten et al., Nat BME, 2018 |\n",
    "\n",
    "--------------------------------------------------\n",
    "The /code/aggregate_models/CRISPR_Net_weights.h5 was trained on dataset I-1, II-1, II-2, and II-4.\n",
    "\n",
    "The /code/scoring_models/CRISPR_Net_CIRCLE_elevation_SITE_weights.h5 was trained on dataset I-1, II-1, II-2, II-3, and II-4.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b5d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/users/zzhang/CRISPR_pred/crispr_kinn\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8acd98ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "import os\n",
    "from collections import defaultdict\n",
    "# we override the sequence encoder for our KINN use\n",
    "from src.encode_seq import Encoder\n",
    "from src.data import load_finkelstein_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9359b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./baselines/CRISPR_Net/data/\"\n",
    "DAT_I = \"Dataset_I_indel_mismatch\"\n",
    "DAT_II = \"Dataset_II_mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ced8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "def split_data_by_grnas(on_seq_to_idx, codes, labels, split_ratio=0.2):\n",
    "    n_grna = len(on_seq_to_idx)\n",
    "    leave_out_gr = np.random.choice([k for k in on_seq_to_idx], int(np.ceil(n_grna*split_ratio)), replace=False)\n",
    "    leave_out_idx = np.concatenate([on_seq_to_idx[x] for x in leave_out_gr])\n",
    "    train_x, valid_x = np.delete(codes, leave_out_idx, axis=0), codes[leave_out_idx]\n",
    "    train_y, valid_y = np.delete(labels, leave_out_idx, axis=0), labels[leave_out_idx]\n",
    "    print(f\"Split total n_grna {n_grna}, n_pos {np.sum(labels, dtype=int)}, train datapoints {len(train_x)} / {np.sum(train_y, dtype=int)}, valid datapoints {len(valid_x)} / {np.sum(valid_y, dtype=int)}\")\n",
    "    return (train_x, train_y), (valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f263ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training\n",
    "# I-1\n",
    "def load_CIRCLE_data():\n",
    "    print(\"Encoding CIRCLE-seq dataset (dataset I/1)...\")\n",
    "    circle_data = pd.read_csv(f\"{DATA_DIR}/{DAT_I}/dataset_I-1/CIRCLE_seq_10gRNA_wholeDataset.csv\")\n",
    "    circle_codes = []\n",
    "    circle_labels = []\n",
    "    on_seq_to_idx = defaultdict(list)\n",
    "    i = 0\n",
    "    for idx, row in circle_data.iterrows():\n",
    "        on_seq = row['sgRNA_seq']\n",
    "        off_seq = row['off_seq']\n",
    "        # keep on_seq as keys\n",
    "        on_seq_key = on_seq.replace('-', '').replace('_', '')\n",
    "        on_seq_to_idx[on_seq_key].append(i)\n",
    "        i += 1\n",
    "        label = row['label']\n",
    "        read_val = row['Read']\n",
    "        en = Encoder(on_seq=on_seq, off_seq=off_seq, with_category=True, label=label)\n",
    "        en.encode_on_off()\n",
    "        circle_codes.append(en.on_off_code)\n",
    "        circle_labels.append(label)\n",
    "    circle_codes = np.array(circle_codes)\n",
    "    circle_labels = np.array(circle_labels)\n",
    "    print(\"Finished!\", \"Dataset size:\", circle_codes.shape, len(circle_labels[circle_labels>0]))\n",
    "    train, valid = split_data_by_grnas(on_seq_to_idx=on_seq_to_idx, codes=circle_codes, labels=circle_labels)\n",
    "    #return circle_codes, circle_labels\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "# II-1\n",
    "def load_elevation_CD33_dataset():\n",
    "    print(\"Loading dataset II/1...\")\n",
    "    cd33_data = pd.read_pickle(f\"{DATA_DIR}/{DAT_II}/\"\n",
    "                               + \"/Listgarten_ElevationDataset-dataset_II-1_II-2_II-4/cd33_dataset_II-1.pkl\")\n",
    "    cd33_mut = cd33_data[0]\n",
    "    cd33_code = []\n",
    "    label = []\n",
    "    # set up on-seq recorder\n",
    "    on_seq_to_idx = defaultdict(list)\n",
    "    i = 0\n",
    "    for idx, row in cd33_mut.iterrows():\n",
    "        on_seq = row['30mer']\n",
    "        off_seq = row['30mer_mut']\n",
    "        # keep on_seq as keys\n",
    "        on_seq_key = on_seq.replace('-', '').replace('_', '')\n",
    "        on_seq_to_idx[on_seq_key].append(i)\n",
    "        i += 1\n",
    "        etp_val = row['Day21-ETP']\n",
    "        etp_label = row['Day21-ETP-binarized']\n",
    "        en = Encoder(on_seq=on_seq, off_seq=off_seq, with_reg_val=True, value=etp_val)\n",
    "        en.encode_on_off()\n",
    "        cd33_code.append(en.on_off_code)\n",
    "        label.append(etp_label)\n",
    "    label = np.array(label)\n",
    "    cd33_code = np.array(cd33_code)\n",
    "    print(\"Finished!\", cd33_code.shape, len(label[label>0]))\n",
    "    train, valid = split_data_by_grnas(on_seq_to_idx=on_seq_to_idx, codes=cd33_code, labels=np.array(label))\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "# II-2\n",
    "def load_elevation_hmg_dataset():\n",
    "    print(\"Loading dataset II/2...\")\n",
    "    hmg_data = pd.read_pickle(f\"{DATA_DIR}/{DAT_II}/Listgarten_ElevationDataset-dataset_II-1_II-2_II-4/hmg_data_dataset_II-2.pkl\")\n",
    "    hmg_code = []\n",
    "    hmg_vals = []\n",
    "    # set up on-seq recorder\n",
    "    on_seq_to_idx = defaultdict(list)\n",
    "    i = 0\n",
    "    for idx, row in hmg_data.iterrows():\n",
    "        on_seq = row['30mer']\n",
    "        off_seq = row['30mer_mut']\n",
    "        # keep on_seq as keys\n",
    "        on_seq_key = on_seq.replace('-', '').replace('_', '')\n",
    "        on_seq_to_idx[on_seq_key].append(i)\n",
    "        i += 1\n",
    "        reg_val = row['readFraction']\n",
    "        en = Encoder(on_seq=on_seq, off_seq=off_seq, with_reg_val=True, value=reg_val)\n",
    "        en.encode_on_off()\n",
    "        hmg_code.append(en.on_off_code)\n",
    "        hmg_vals.append(en.value)\n",
    "\n",
    "    hmg_vals = np.array(hmg_vals)\n",
    "    hmg_code = np.array(hmg_code)\n",
    "    hmg_label = np.zeros(len(hmg_vals))\n",
    "    hmg_label[hmg_vals>0] = 1\n",
    "    print(\"Finished!\", \"dataset size: \", hmg_code.shape, len(hmg_label[hmg_label>0]))\n",
    "    train, valid = split_data_by_grnas(on_seq_to_idx=on_seq_to_idx, codes=np.array(hmg_code), labels=hmg_label)\n",
    "    return train, valid\n",
    "    \n",
    "\n",
    "# II-3\n",
    "def load_siteseq_data():\n",
    "    print(\"Loading SITE-Seq dataset (dataset II/3) .....\")\n",
    "    siteseq_data = pd.read_csv(f\"{DATA_DIR}/{DAT_II}/dataset_II-3/SITE-Seq_offTarget_wholeDataset.csv\", index_col=0)\n",
    "    code = []\n",
    "    reads = []\n",
    "    # set up on-seq recorder\n",
    "    on_seq_to_idx = defaultdict(list)\n",
    "    i = 0\n",
    "    for idx, row in siteseq_data.iterrows():\n",
    "        on_seq = '-'+row['on_seq'].upper()\n",
    "        off_seq = '-'+row['off_seq'].upper()\n",
    "        # keep on_seq as keys\n",
    "        on_seq_key = on_seq.replace('-', '').replace('_', '')\n",
    "        on_seq_to_idx[on_seq_key].append(i)\n",
    "        i += 1\n",
    "        en = Encoder(on_seq=on_seq, off_seq=off_seq, with_reg_val=True, value=row['reads'])\n",
    "        en.encode_on_off()\n",
    "        code.append(en.on_off_code)\n",
    "        reads.append(en.value)\n",
    "    code = np.array(code)\n",
    "    reads = np.array(reads)\n",
    "    labels = np.zeros(len(reads))\n",
    "    labels[reads > 0] = 1\n",
    "    print(len(on_seq_to_idx), code.shape, len(labels[labels>0]))\n",
    "    train, valid = split_data_by_grnas(on_seq_to_idx=on_seq_to_idx, codes=code, labels=labels)\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "# II-4\n",
    "def load_elevation_guideseq_data():\n",
    "    print(\"Loading dataset II/4...\")\n",
    "    guideseq_data = pd.read_pickle(f\"{DATA_DIR}/{DAT_II}/Listgarten_ElevationDataset-dataset_II-1_II-2_II-4/guideseq_data_dataset_II-3.pkl\")\n",
    "    guideseq_code = []\n",
    "    guideseq_vals = []\n",
    "    # set up on-seq recorder\n",
    "    on_seq_to_idx = defaultdict(list)\n",
    "    i = 0\n",
    "    for idx, row in guideseq_data.iterrows():\n",
    "        on_seq = row['30mer']\n",
    "        off_seq = row['30mer_mut']\n",
    "        reg_val = row['GUIDE-SEQ Reads']\n",
    "        # keep on_seq as keys\n",
    "        on_seq_key = on_seq.replace('-', '').replace('_', '')\n",
    "        on_seq_to_idx[on_seq_key].append(i)\n",
    "        i += 1\n",
    "        en = Encoder(on_seq=on_seq, off_seq=off_seq, with_reg_val=True, value=reg_val)\n",
    "        en.encode_on_off()\n",
    "        guideseq_code.append(en.on_off_code)\n",
    "        guideseq_vals.append(en.value)\n",
    "\n",
    "    guideseq_code = np.array(guideseq_code)\n",
    "    guideseq_vals = np.array(guideseq_vals)\n",
    "    guideseq_labels = np.zeros(len(guideseq_vals))\n",
    "    guideseq_labels[guideseq_vals > 0] = 1\n",
    "    print(\"Dataset size:\", guideseq_code.shape, \"positive num:\", len(guideseq_labels[guideseq_labels > 0]))\n",
    "    train, valid = split_data_by_grnas(on_seq_to_idx=on_seq_to_idx, codes=np.array(guideseq_code), labels=np.array(guideseq_labels))\n",
    "    return train, valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cade6281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding CIRCLE-seq dataset (dataset I/1)...\n",
      "Finished! Dataset size: (584949, 25, 13) 7371\n",
      "Split total n_grna 40, n_pos 7371, train datapoints 417555 / 5593, valid datapoints 167394 / 1778\n",
      "Loading dataset II/1...\n",
      "Finished! (4853, 25, 13) 2273\n",
      "Split total n_grna 1027, n_pos 2273, train datapoints 3819 / 1853, valid datapoints 1034 / 420\n",
      "Loading dataset II/2...\n",
      "Finished! dataset size:  (10129, 25, 13) 52\n",
      "Split total n_grna 19, n_pos 52, train datapoints 7968 / 45, valid datapoints 2161 / 7\n",
      "Loading SITE-Seq dataset (dataset II/3) .....\n",
      "9 (217733, 25, 13) 3767\n",
      "Split total n_grna 9, n_pos 3767, train datapoints 180000 / 2799, valid datapoints 37733 / 968\n",
      "Loading dataset II/4...\n",
      "Dataset size: (294534, 25, 13) positive num: 354\n",
      "Split total n_grna 36, n_pos 354, train datapoints 232956 / 276, valid datapoints 61578 / 78\n"
     ]
    }
   ],
   "source": [
    "d1 = load_CIRCLE_data()\n",
    "d2 = load_elevation_CD33_dataset()\n",
    "d3 = load_elevation_hmg_dataset()\n",
    "d4 = load_siteseq_data()\n",
    "d5 = load_elevation_guideseq_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5cd284d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((842298, 25, 13), (842298,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.concatenate([x[0][0] for x in (d1, d2, d3, d4, d5)]), np.concatenate([x[0][1] for x in (d1, d2, d3, d4, d5)])\n",
    "train_data[0].shape, train_data[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f397740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((269900, 25, 13), (269900,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data = np.concatenate([x[1][0] for x in (d1, d2, d3, d4, d5)]), np.concatenate([x[1][1] for x in (d1, d2, d3, d4, d5)])\n",
    "valid_data[0].shape, valid_data[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "883458f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid positive size 6976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((13993, 25, 13), (13993,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use the kinetic finkelstein data as validation set\n",
    "# to compute NAS rewards\n",
    "t1, _ = load_finkelstein_data(target='wtCas9_cleave_rate_log', make_switch=False, logbase=10, include_ref=True)\n",
    "t2, _ = load_finkelstein_data(target='wtCas9_cleave_rate_log', make_switch=True, logbase=10, include_ref=True)\n",
    "\n",
    "x_valid = np.concatenate([t1[0], t2[0]])\n",
    "k_valid = np.concatenate([t1[1], t2[1]])\n",
    "kinetic_label = np.zeros(len(k_valid))\n",
    "kinetic_label[k_valid > -5] = 1\n",
    "\n",
    "kinetic_data = x_valid, k_valid, kinetic_label\n",
    "print(\"valid positive size\", (kinetic_label==1).sum())\n",
    "kinetic_data[0].shape, kinetic_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173d9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing\n",
    "# I-2 is missing in the original code pod, but present in dataset\n",
    "def load_listgarten_indel_dataset():\n",
    "    print(\"Loading Listgarten indel dataset (dataset I/2)...\")\n",
    "    df = pd.read_csv(f\"{DATA_DIR}/{DAT_I}/dataset_I-2/elevation_6gRNA_wholeDataset.csv\")\n",
    "    code = []\n",
    "    labels = []\n",
    "    for idx, row in df.iterrows():\n",
    "        on_seq = row['crRNA'].upper()\n",
    "        off_seq = row['DNA'].upper()\n",
    "        #  print(idx, on_seq)\n",
    "        label = row['label']\n",
    "        en = Encoder(on_seq=on_seq, off_seq=off_seq, with_category=True, label=label)\n",
    "        en.encode_on_off()\n",
    "        code.append(en.on_off_code)\n",
    "        labels.append(en.label)\n",
    "    labels = np.array(labels)\n",
    "    code = np.array(code)\n",
    "    print(\"Finished!\")\n",
    "    print(code.shape, len(labels[labels > 0]))\n",
    "    return code, labels\n",
    "\n",
    "    \n",
    "# II-5\n",
    "def load_Kleinstiver_data():\n",
    "    print(\"Loading Kleinsitver dataset (dataset II/5)...\")\n",
    "    sgRNA5_data = pd.read_csv(f\"{DATA_DIR}/{DAT_II}/dataset_II-5/Kleinstiver_5gRNA_wholeDataset.csv\")\n",
    "    sgRNA5_code = []\n",
    "    sgRNA5_labels = []\n",
    "    for idx, row in sgRNA5_data.iterrows():\n",
    "        on_seq = row['sgRNA_seq'].upper()\n",
    "        off_seq = row['off_seq'].upper()\n",
    "        #  print(idx, on_seq)\n",
    "        label = row['label']\n",
    "        en = Encoder(on_seq=on_seq, off_seq=off_seq, with_category=True, label=label)\n",
    "        en.encode_on_off()\n",
    "        sgRNA5_code.append(en.on_off_code)\n",
    "        sgRNA5_labels.append(en.label)\n",
    "    sgRNA5_labels = np.array(sgRNA5_labels)\n",
    "    sgRNA5_code = np.array(sgRNA5_code)\n",
    "    print(\"Finished!\")\n",
    "    print(sgRNA5_code.shape, len(sgRNA5_labels[sgRNA5_labels > 0]))\n",
    "    return sgRNA5_code, sgRNA5_labels\n",
    "\n",
    "\n",
    "# II-6\n",
    "def load_22sgRNA_data():\n",
    "    print(\"Loading Listgarten dataset II/6...\")\n",
    "    sgRNA22_data = pd.read_csv(f\"{DATA_DIR}/{DAT_II}/dataset_II-6/Listgarten_22gRNA_wholeDataset.csv\")\n",
    "    sgRNA22_code = []\n",
    "    sgRNA22_labels = []\n",
    "    for idx, row in sgRNA22_data.iterrows():\n",
    "        on_seq = row['sgRNA_seq'].upper()\n",
    "        # print(idx, on_seq)\n",
    "        off_seq = row['off_seq'].upper()\n",
    "        label = row['label']\n",
    "        en = Encoder(on_seq=on_seq, off_seq=off_seq, with_category=True, label=label)\n",
    "        en.encode_on_off()\n",
    "        sgRNA22_code.append(en.on_off_code)\n",
    "        sgRNA22_labels.append(en.label)\n",
    "    sgRNA22_labels = np.array(sgRNA22_labels)\n",
    "    sgRNA22_code = np.array(sgRNA22_code)\n",
    "    print(\"Finished!\", \"Dataset size: \", np.array(sgRNA22_code).shape, len(sgRNA22_labels[sgRNA22_labels > 0]))\n",
    "    return np.array(sgRNA22_code), np.array(sgRNA22_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03961292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Listgarten indel dataset (dataset I/2)...\n",
      "Finished!\n",
      "(213943, 25, 13) 60\n",
      "Loading Kleinsitver dataset (dataset II/5)...\n",
      "Finished!\n",
      "(95829, 25, 13) 54\n",
      "Loading Listgarten dataset II/6...\n",
      "Finished! Dataset size:  (383463, 25, 13) 56\n"
     ]
    }
   ],
   "source": [
    "t1 = load_listgarten_indel_dataset()\n",
    "t2 = load_Kleinstiver_data()\n",
    "t3 = load_22sgRNA_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb5883c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((693235, 25, 13), (693235,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data is actually sperate evaluated within each dataset\n",
    "test_data = np.concatenate([x[0] for x in (t1, t2, t3)]), np.concatenate([x[1] for x in (t1, t2, t3)])\n",
    "test_data[0].shape, test_data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49a5ee9",
   "metadata": {},
   "source": [
    "## Store Data in h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c1d7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"./data/inVivoData.newValidSplit.h5\", \"w\") as store:\n",
    "    train = store.create_group(\"train\")\n",
    "    train.create_dataset(\"x\", data=train_data[0])\n",
    "    train.create_dataset(\"y\", data=train_data[1])\n",
    "    \n",
    "    valid = store.create_group(\"valid\")\n",
    "    valid.create_dataset(\"x\", data=valid_data[0])\n",
    "    valid.create_dataset(\"y\", data=valid_data[1])\n",
    "\n",
    "    kinetic = store.create_group(\"kinetic\")\n",
    "    kinetic.create_dataset(\"x\", data=kinetic_data[0])\n",
    "    kinetic.create_dataset(\"k\", data=kinetic_data[1])\n",
    "    kinetic.create_dataset(\"y\", data=kinetic_data[2])\n",
    "\n",
    "    store.create_dataset(\"test/Listgarten_indel/x\", data=t1[0])\n",
    "    store.create_dataset(\"test/Listgarten_indel/y\", data=t1[1])\n",
    "\n",
    "    store.create_dataset(\"test/Kleinsitver_mut/x\", data=t2[0])\n",
    "    store.create_dataset(\"test/Kleinsitver_mut/y\", data=t2[1])\n",
    "    \n",
    "    store.create_dataset(\"test/Listgarten_mut/x\", data=t3[0])\n",
    "    store.create_dataset(\"test/Listgarten_mut/y\", data=t3[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5e3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
